---
title: "A Machine Learning Algorithim for Predicting Human Activity"
author: "Davin O'Regan"
date: "8/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
testPC <- read.csv("/Users/davinoregan30/Documents/data/pml/data/testPC.csv")
phat_rf <- read.csv("/Users/davinoregan30/Documents/data/pml/data/phat_rf.csv")
phat_vd2 <- read.csv("/Users/davinoregan30/Documents/data/pml/data/phat_vd2.csv")
training4 <- read.csv("/Users/davinoregan30/Documents/data/pml/data/training4.csv")
```

## Introduction

This brief note describes the training of a machine learning model to predict human activity from the  [human activity recognition data (HAR) initiative](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har). The model employed a random forest analytic technique using training data obtained from the HAR website. Prior to model fitting, the initial data set was split into separate training and test sets, the training data was processed using principal components analysis, and the modeling technique employed repeated cross validation with 10 folds to model accuracy. Additionally, a separate set of 20 test cases was predicted, and the results were submitted online in separate format. I discuss each of my model preparation and execution steps in turn.

## Data Preparation and Pre-Processing

First, I inspected the raw training data using cross tabs and histogram plots and found that large numbers of variables contained entirely empty or nonsensical observations. The missing observations were so extensive that imputation would be verge on data fabrication. These "NA", empty, or otherwise uninterpretable observations were dropped from the training set. This reduced the number of variables in the data from 160 to 60. 

Next, I split the training data set into its own training and test sets so that I would be able to better assess the out-of-sample error of my model before applying the model to the 20 test cases provided for this exercise. This produced a training set of 14,718 observations (70% of the initial training set) and a test set of 4,904.

The data still contained a large number of variables and little information was provided about what they measured. The below heat map reflects the correlations across most continuous variables in the data set. As can be seen by the variable names and the heat map, little can be interpreted about their purpose or relationship to the outcome and no specific relationships stand out. I therefore opted to process the remaining set of 60 variables using principal components analysis to create a more succinct set of predictors that still captured 95% of the variation in the training data set. The result was a set of 27 principal components on which to train a model. This PCA pre-processing was applied both to the training and validation data sets.

````{r, echo=F, fig.align = 'center'}
cormat <- cor(training4[,8:59])
melted_cormat <- reshape2::melt(cormat)
library(ggplot2)
g <- ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
    geom_tile() +
  scale_fill_gradient2(low="green", mid="yellow", high="red")
g
````

## Model Selection

I employed a random forest technique to model the type of human activity in the data. Random forest is an ideal technique for outcomes that are multi-class categorical variables and is useful for predicting outcomes (while being less useful for inferring and measuring relationships between independent and dependent variables). The random forest procedure is also useful when less is known about the features available for prediction, since the technique will privilege those features that split the data in the most optimal way. 

To improve the accuracy of our model, 10-fold repeated cross-validation was employed in the random forest technique. Cross validation improves the accuracy of the model and minimizes bias in the results. Repeated cross folding reduces the noise that may result from differing splits in the data. Only 10 folds were used so that model variance would remain low.

Given that the random forest model used PCA data, providing model output or plots is of limited value and so none are displayed here.

## Out-of-Sample Error

Once the random forest model was trained, the out-of sample error was computed against the test set that had been split from the original training data. The model correctly classified over 99 percent of the 4,904 observations that had been split from the original training data. The predicted and actual outcomes in the test set can be seen below.

````{r,  echo=F}
knitr::kable(phat_rf, row.names=F,
           caption = "Table 2. Predicted vs. Actual Outcomes, Test Set") 
````
